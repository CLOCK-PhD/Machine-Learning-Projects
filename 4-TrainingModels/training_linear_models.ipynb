{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapitre 4 - Entraîner des modèles linéaires\n",
    "\n",
    "On va aller voir de manière plus approfondie comment les choses fonctionnent et comment choisir un modèle approprié.\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#python >= 3.5 nécessaire\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "#scikit-learn >= 0.20 nécessaire\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "# Autres imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# pour rendre les output stables au fil des runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# pour tracer de jolis graphes\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# où sauvegarder les figures\n",
    "PROJECT_ROOT_DIR = \"..\"\n",
    "CHAPTER_ID = \"training_linear_models\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)\n",
    "\n",
    "# ignorer les warnings inutiles (voir SciPy issue #5998)\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Régression linéaire\n",
    "\n",
    "Un modèle linéaire fait des prédictions en calculant simplement une somme pondérée des caractéristiques en entrée (*input features*), plus une constante appelée terme de biais (*bias term*).\n",
    "\n",
    "*Prédiction du modèle de régression linéaire*\n",
    "$$\n",
    "\\hat{y} = \\theta_{0} + \\theta_{1}x_{1} + \\theta_{1}x_{1} + \\cdots + \\theta_{n}x_{n}\n",
    "$$\n",
    "\n",
    "Dans cette équation :\n",
    "- $\\hat{y}$ est la *valeur prédite*\n",
    "- $n$ est le nombre de caractéristiques (*features*)\n",
    "- $x_{i}$ est la $i^{ème}$ valeur de caractéristique (*feature value*)\n",
    "- $\\theta_{j}$ est le $j^{ème}$ paramètre du modèle (qui inclut le terme de biais $\\theta_{0}$ les poids des caractéristiques (*feature weight*) $\\theta_{1}$, $\\theta_{2}$, ..., $\\theta_{n}$)\n",
    "\n",
    "Cette équation peut être écrite de manière plus concise en utilisant une forme vectorisée :\n",
    "\n",
    "*Prédiction du modèle de régression linéaire (forme vectorisée)*\n",
    "$$\n",
    "\\hat{y} = h_{\\theta}(x) = \\theta \\cdot x\n",
    "$$\n",
    "\n",
    "Dans cette équation :\n",
    "- $\\theta$ est le *vecteur de paramètre* (*parameter vector*) du modèle, contenant le terme de biais $\\theta_{0}$ et la pondération des caractéristiques $\\theta_{1}$ à $\\theta_{n}$.\n",
    "- $x$ est le *vecteur de caractéristique* (*feature vector*), contenant $x_{0}$ à $x_{n}$, avec $x_{0}$ toujours égal à 1\n",
    "- $\\theta \\cdot x$ est le produit scalaire des vecteurs $\\theta$ et $x$, qui est bien sûr égal à $\\theta_{1}x_{1} + \\theta_{1}x_{1} + \\cdots + \\theta_{n}x_{n}$\n",
    "- $h_{\\theta}$ est la fonction d'hypothèse, utilisant les paramètres du modèle $\\theta$.\n",
    "\n",
    "Voilà pour la modèle de régression linéaire. On va maintenant voir comment l'entraîner ; il va avoir besoin qu'on règle ses paramètres pour qu'il corresponde le plus au JdE.\n",
    "\n",
    "Dans ce but, on va d'abord avoir besoin d'une mesure de la façon dont le modèle s'ajuste bien (ou mal) au JdE ; on va utiliser la RMSE (*Root Mean Square Error*) pour ça. Dincn, pour entraîner un modèle de régression linéaire, on va avoir besoin de *trouver la valeur de $\\theta$ qui minimise* le RMSE. Dans la pratique, c'est plus simple de minimser la MSE (*Mean Squared Errpr*) que la RMSE, et ça conduit au même résultat (parce que la valeur qui minimise une fonction minimise aussi sa racine carrée).\n",
    "\n",
    "La MSE d'une hypothèse $h_{\\theta}$ d'un JdE X est calculé selon l'équation suivante :\n",
    "\n",
    "$$\n",
    "MSE(X, h_{\\theta}) = \\frac{1}{m}\\sum_{i=1}^{m}(\\theta^{⊺}x^{(i)}-y^{(i)})^2\n",
    "$$\n",
    "\n",
    "On écrit $h_{\\theta}$ au lieu de $h$ pour être clair que le modèle est paramétré par le vecteur $\\theta$. Pour simplifier les notations, on va écrire MSE($\\theta$) au lieu de $MSE(X, h_{\\theta})$\n",
    "\n",
    "\n",
    "### L'équation normale (the normal equation)\n",
    "\n",
    "Pour trouver la valeur de $\\theta$ qui minimise la fonction de coût, il y a une solution en forme fermée (*closed form solution*), en d'autres mots, une equation mathématique qui donne le résultat directement. C'est ce qu'on appelle l'équation normale :\n",
    "\n",
    "$$\n",
    "\\hat{\\theta} = (X^{⊺}X)^{-1} X^{⊺} y\n",
    "$$\n",
    "\n",
    "Dans cette équation :\n",
    "- $\\hat{\\theta}$ est la valeur de $\\theta$ qui minimise la fonction de coût\n",
    "- $y$ est le vecteur des valeurs cibles contenant $y^{1}$ à $y^{m}$\n",
    "\n",
    "On va maintenant générer des données qui ressemblent à quelque chose de linéaire pour tester cette equation :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving figure generated_data_plot\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = 2 * np.random.rand(100, 1)\n",
    "y = 4 + 3 * X + np.random.randn(100, 1)\n",
    "\n",
    "# tracer le graphe\n",
    "plt.plot(X, y, \"b.\")\n",
    "plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "plt.ylabel(\"y\", rotation=0, fontsize=18)\n",
    "plt.axis([0, 2, 0 ,15])\n",
    "save_fig(\"generated_data_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant, on va calculer $\\hat{\\theta}$ en utilisant l'équation normale. Onva utiliser la fonction `inv()` du module d'algèrbe linéaire de NumPy (`np.linalg`) pour calculer l'inverse d'une matrice, et la méthode `dot()` pour la multiplication des matrices :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_b = np.c_[np.ones((100,1)), X]    # ajoute x0 = 1 à chaque instance\n",
    "theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction qu'on a utilisé pour générer les données est $y = 4 + 3x_{i} + \"bruit gaussien\"$. Voyons voir ce que l'équation trouve :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.9202609],\n",
       "       [2.9609808]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On aurait aimé trouver $\\theta_{0}=4$ et $\\theta_{1}=3$. On a un résultat assez proche mais le bruit rend impossible la découverte des paramètres exacts de la fonction d'origine.\n",
    "\n",
    "Maintenant, on peut faire des prédictions en utilisant $\\hat{\\theta}$ :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.9202609],\n",
       "       [9.8422225]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = np.array([[0], [2]])\n",
    "X_new_b = np.c_[np.ones((2, 1)), X_new] # ajoute x0 = 1 à chaque instance\n",
    "y_predict = X_new_b.dot(theta_best)\n",
    "y_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va maintenant tracer les prédictions du modèle :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving figure linear_model_predictions_plot\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(X_new, y_predict, \"r-\", linewidth=2, label=\"Predictions\")\n",
    "plt.plot(X, y, \"b.\")\n",
    "plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
    "plt.legend(loc=\"upper left\", fontsize=14)\n",
    "plt.axis([0, 2, 0, 15])\n",
    "save_fig(\"linear_model_predictions_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Réaliser une Régression Linéaire en utilisant Scikit-Learn est simple (à noter que sklearn sépare le terme de biais (`intercept_`) des poids des caractéristiques (*feature weights*)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3.9202609]), array([[2.9609808]]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X, y)\n",
    "lin_reg.intercept_, lin_reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.9202609],\n",
       "       [9.8422225]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_reg.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La classe `LinearRegression` est basée sur la fonction `scipy.linalg.lstsq()` (*least square*), qu'on peut appeler directement :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.9202609],\n",
       "       [2.9609808]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_best_svd, residuals, rank, s = np.linalg.lstsq(X_b, y, rcond=1e-6)\n",
    "theta_best_svd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette fonction calcule $\\hat{\\theta} = X^{+}y$ où $X^{+}$ est le *pseudoinverse* de X (l'inverse de Moore-Penrose). On peut utiliser `np.linalg.pinv()` pour calculer le pseudoinverse directement :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.9202609],\n",
       "       [2.9609808]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.pinv(X_b).dot(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le pseudoinverse en lui-même est calculé à l'aide d'une technique de factorisation matricielle standard appelée SVD (*Standard Value Decomposition*) qui peut décomposer la matrice du JdE **X** en une matrice de multiplication de trois matrices $U \\sum V^{⊺}$ (voir `numpy.linalg.svd()`).\n",
    "\n",
    "Le pseudoinverse est calculé comme $X^{+} = \\sum^{+}U^{⊺}$. Pour calculer la matrice **$\\sum^{+}$**, l'algorithme prend **$\\sum$** et mets à 0 toutes les valeurs plus petites qu'une minuscule valeur seuil, et remplace toutes les valeurs différentes de 0 par leur inverse, pour finalement les transpoer dans la matrice résultante.\n",
    "\n",
    "Cette approche est plus efficace que de calculer l'équiation normale, et permet se charger des cas extrêmes d'une bonne manière : en effet, l'équation normale ne marche pas si la matrice $X^{⊺}X$ n'est pas inversible (c'est à dire singulière), comme si m < n ou si certaines caractéristiques (*features*) sont redondantes, mais que la pseudo-inverse est toujours définie.\n",
    "\n",
    "### Complexité Computationnelle\n",
    "\n",
    "L'équation normale caclue l'inverse de $X^{⊺}X$, qui est une matrice $(n+1)*(n+1)$ (où $n$ est le nombre de caractéristiques (*features*)). La *complexité computationnelle* de l'inversion d'une telle matrice est typiquement en $O(n^{2.4}$ ou $O(n^{3}$, dépendemment de l'implémentation. En d'autres termes, si on double le nombre de caractéristiques (*features*), on multiplie le temps de calcul par environ $2^{2.4} = 5.3$ à $2^{3} = 8$.\n",
    "\n",
    "L'approche SVD utilisée par la classe `LinearRegression` de Scikit-Learn est en $O(n^{2})$. Si on double le nombre de caractéristiques (*features*), on multiplie le temps de calcul par 4.\n",
    "\n",
    "*Attention* : aussi bien l'Équation Normale que l'approche SVD sont très lentes quand le nombre de caractéristiques (*features*) devient grand (environ 100 000). Mais elles sont toutes les deux linéaires en ce qui concerne le nombre d'instances du JdE ($O(m)$), donc elles prennent très bien en charge les grands JdE, à condition qu'on puisse les mettre en mémoire.\n",
    "\n",
    "De plus, une fois qu'on a entraîné nos modèles de Régression Linéaire, les prédictions sont très rapides : la complexité computationnelle est linéaire aussi bien au regard du nombre d'instances qu'on veut prédire ainsi que le nombre de *features*.\n",
    "\n",
    "On va maintenant s'intéresser à d'autres manières bien différentes d'entraîner un modèle de Régression Linéaire, qui seront plus adaptées pour des cas où on a un grand nombre de *features* ou trop d'instances d'entraînement pour rentrer en mémoire.\n",
    "\n",
    "## Descente de gradient (*Gradient descent*)\n",
    "\n",
    "La descente de gradient est un algorithme générique d'optimisation  capable de trouver des solutions optimales à un large champ de problèmes. L'idée générale est de modifier les paramètres de manière itérative afin de minimiser une fonction de coût.\n",
    "\n",
    "Elle mesure le gradient local de la fonction d'erreur par rapport au vecteur paramètre $\\theta$, et va dans le sens du gradient descendant. Une fois la pente nulle, on atteint un minimum.\n",
    "\n",
    "Concrètement, on commence par remplir $\\theta$ de valeurs aléatoires (c'est ce qu'on appelle l'*initialisation aléatoire* (*random initialization*)). Ensuite, on l'améliore graduellement, pas à pas, chaque pas essayant de réduire le coût de la fonction (ex : MSE), jusqu'à ce que l'algorithme *converge* vers un minimum.\n",
    "\n",
    "Un paramètre important de la descente de gradient est la taille des pas, determinée par l'hyperparamètre *taux d'apprentissage* (*learning rate*). Si le taux d'apprentissage est trop petit, alors l'algo va devoir passer par de nombreuses itérations pour converger, ce qui prendra un certain temps.\n",
    "\n",
    "Si le taux d'apprentissage est trop élevé, on peut \"sauter\" de l'autre côté, et même plus haut qu'avant. Cela va faire diverger l'algorithme, avec valeurs de plus en plus larges, l'empêchant de trouver une bonne solution.\n",
    "\n",
    "Enfin, il faut savoir que toutes les fonctions ressemblent à un bol régulier. Il peut y avoir des trous, des crêtes, et des irrégularités, rendant la convergence vers le minimum difficile. On se retrouve parfois un minimum local qui empêche d'accéder au minimum global, en fonction de là où l'algo a commencé, ou encore avec des plateau qui font stagner la recherche.\n",
    "\n",
    "Heureusement, la fonction de coût MSE pour une Régression Linéaire est une *fonction convexe*, ce qui veut dire que si on prend au hasard deux points dans la courbe, la ligne qui le rejoint ne va jamais croiser la courbe. Cela implique qu'il y a seulement un minimum global et pas de minimum local. C'est également une fonction avec une pente qui ne change jamais de manière abrupte. Ces deux faits ont une très bonne conséquence : la Descente de Gradient garanti qu'on va approcher arbitrairement du minimum global (si on attend suffisamment longtemps et si le taux d'apprentissage n'est pas trop élevé).\n",
    "\n",
    "En fait, la fonction de coût a la forme d'un bol qui peut être allongé si les *features* ont des échelles très différentes, ce qui va faire que dans ce cas, il mettra beaucoup plus de temps à atteindre le minimum global. Quand on utilse la Descente de Gradient, il vaut mieux s'assurer qu'on a des *features* avec les mêmes échelles pour éviter ce problème.\n",
    "\n",
    "Entraîner un modèle implique de chercher une combinaison de paramètres du modèle qui minimisent une fonction de coût (sur le JdE). C'est une recherche dans l'*espace des paramètres* : plus un modèle a de paramètres, plus l'espace a de dimensions, et plus dure est la recherche.\n",
    "\n",
    "### Descente de gradient de lot (*Batch Gradient Descent*)\n",
    "\n",
    "Pour implémenter la descente de gradient, on doit calculer le gradient de la fonction de coût pour chaque paramètre $\\theta_{j}$ du modèle. Autrement dit, on doit calculer de combien va changer la fonction de cout si on change $\\theta_{j}$ juste un peu. C'est ce qu'on appelle une *dérivée partielle*.\n",
    "\n",
    "L'équation suivante calcule la dérivée partielle de la fonction de coût par rapport au paramètre $\\theta_{j}$, noté $\\partial MSE(\\theta) / \\partial{\\theta_{j}}$ :\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial{\\theta_{j} }}MSE(\\theta) = \\frac{2}{m}\\sum_{i=1}^{m}(\\theta^{⊺}x^{(i)} - y^{j})x_{j}^{i}\n",
    "$$\n",
    "\n",
    "Au lieu de calculer ces dérivées partielles individuellement, on peut toutes les calculer d'un coup. Le vecteur gradiant, noté $∇_{\\theta} MSE(\\theta)$ contient toutes les dérivées partielles de la fonction de coût (une pour chaque paramètre du modèle).\n",
    "\n",
    "Une fois qu'on a notre vecteur gradient, qui pointe en haut, il faut juste aller dans l'autre direction et aller vers le bas. Cela implique de soustraire $∇_{\\theta} MSE(\\theta)$ de $\\theta$. C'est ici que le taux d'apprentissage $\\eta$ rentre en jeu : multiplier le vecteur gradient par $\\eta$ pour déterminer la taille du pas vers le bas.\n",
    "\n",
    "Regardons l'implémentation de cet algorithme :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.9202609],\n",
       "       [2.9609808]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eta = 0.1   # taux d'apprentissage\n",
    "n_iterations = 1000\n",
    "m = 100\n",
    "\n",
    "theta = np.random.randn(2, 1)   # initialisation aléatoire\n",
    "\n",
    "for iterations in range(n_iterations):\n",
    "    gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)\n",
    "    theta = theta - eta * gradients\n",
    "\n",
    "theta"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
